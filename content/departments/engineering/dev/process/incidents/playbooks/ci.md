# Continuous integration playbook

- **Maintainers**: [DevInfra Team](../../../../teams/devinfra/index.md).
- **Audience**: any software engineer, no prior infrastructure knowlegde required.
- **TL;DR** This document sums up what to do in various scenarios that can block the CI.

Sourcegraph's [continuous integration (CI)](https://docs.sourcegraph.com/dev/background-information/continuous_integration) is what enables us to feel confident when delivering our changes to our users, and is one of the key components enabling Sourcegraph to deliver quality software. While the DevInfra team is in charge of managing the CI as a tool, it is essential for every engineer to be able to unblock themselves if there is a problem in order be autonomous.

This page lists common failure scenarios and provides a step by step guide to get the CI back in an operational state.

## Prerequisites

In order to handle problems with the CI, the following elements are necessary:

1. Have access to the `sourcegraph-ci` _project_ on Google Cloud Platform.
1. Ask #it-tech-ops for access if you do not have access.
1. Have the `gcloud` CLI installed.
1. Have the `kubectl` CLI installed.
1. [Gain access to the CI cluster by authenticating against it with `gcloud` and `kubectl`](../../deployments/debugging/tutorial.md#ci-cluster).
1. Request access to the DevX Day2Day entitle bundle by typing `/access_request` in Slack.

> [!NOTE] Optional, additional tips:
>
> - ðŸ’¡ You can set the default namespace to `buildkite` or `buildkite-bazel` to avoid always appending the `-n buildkite` flag to `kubectl` commands.
>   - `kubectl config set-context --current --namespace=buildkite`
> - You can also use [k9s](https://k9scli.io) for easier interactions with the _pods_.

## Scenarios

> [!NOTE] All scenario guides assume you have the appropriate [prerequisites](#prerequisites) set up.

### `buildchecker` has locked the `main` branch

- Severity: _major_
- Impact:
  - No pull requests may be merged except by authors of
  - Pull request builds may be failing as well
- Possible causes:
  - `buildchecker` will lock/restrict push access to the `main` branch if a series of failed builds is detected - this can indicate that a regression has been merged into `main` or that critical build infrastructure is failing.

#### Actions

`buildchecker` will still allow the authors of the last few failed builds, as well as the @dev-infra team, to push to the `main` branch so as to make any changes necessary to restore the pipeline to a healthy state.

1. Follow the ["Build has failed on the `main` branch" guide](#build-has-failed-on-the-main-branch).
2. If the issue has been resolved, wait for `buildchecker` to unlock the branch or [manually trigger a run (click "Run workflow")](https://github.com/sourcegraph/sourcegraph/actions/workflows/buildchecker.yml).

> [!NOTE] If there is a bug in `buildchecker`, a repository admin can go to the [Branches settings](https://github.com/sourcegraph/sourcegraph/settings/branches) -> `main` -> uncheck/check "Restrict who can push to matching branches" -> "Save changes" to override the restrictions.
>
> Also see [the `buildchecker` source code](https://github.com/sourcegraph/sourcegraph/tree/main/dev/buildchecker) and [workflow definition](https://github.com/sourcegraph/sourcegraph/blob/main/.github/workflows/buildchecker.yml).

### Build has failed on the `main` branch

- Severity: _minor_
- Impact: that commit won't be deployed on `k8s.sgdev.org` and `sourcegraph.com` until an ulterior build passes.
- Possible causes:
  - The `main` branch runs additional checks compared to Pull Requests builds. So it's possible that one of those checks failed.
    - ðŸ’¡ The checks are dynamically generated by our [pipeline generation tool](https://docs.sourcegraph.com/dev/background-information/continuous_integration). The `main` branch has notably much more exhaustive checks than other branches.
  - The `main` branch have changes that weren't in the Pull Request branch and those changes are causing a failure.
  - The `main` branch is failing due to a previous build.

#### Actions

1. Check your build on [Buildkite](https://buildkite.com/sourcegraph/sourcegraph/builds?branch=main).
   - Find its link directly in the #buildkite-main channel.
   - ðŸ’¡ Or run `sg ci status` in your shell, with the `main` branch checked out.
2. Search for the failing steps, and browse the logs (ðŸ’¡ run `sg ci logs` in your shell, with the `main` branch checked out) .
   - Look for a failure explanation: it can be a test that failed or a command that return a non zero exit code.
3. Check the previous builds on the `main` branch on [Buildkite](https://buildkite.com/sourcegraph/sourcegraph/builds?branch=main)
   1. Are they failing with the same exact error?
      - **Yes**: see the [Builds are failing in the `main` branch with the same error](#builds-are-all-failing-on-the-main-branch-with-the-same-error)
      - **No**: see next point.
4. Is that a real failure or a flake?
   1. Restart that step. Maybe it will fail again, but if it doesn't it'll save you time.
      - ðŸ’¡ You can go to 3. while it runs.
   1. See [Is that a failure or a flake scenario](#is-this-a-failure-or-a-flake)
   1. Did restarting it fixed the problem?
      - **Yes**: that's a flake. See the [Spotted a flake scenario](#spotted-a-flake)
      - **No**: see next point.
   1. Does the failure points to problem with the code that was shipped on that commit?
      1. Yes, and it's a very quick fix that can get merged promptly:
         1. Write a short message on #buildkite-main and tell others that you're fixing it.
         1. Submit the fix with another PR and get it merged as soon as possible.
      1. Yes, but it's not easily and/or quickly fixed
         1. Revert the incriminating Pull Request.
         1. [Open a GitHub issue](https://github.com/sourcegraph/sourcegraph/issues/new?assignees=&labels=testing%2Cflake&template=flaky_test.md&title=Flake%3A+%24TEST_NAME+disabled) mentioning the build and the context to explain to the team owning that test what happened.
         1. Checkout the PR branch.
         1. Rebase it so it includes the changes that broke it when merged in the `main` branch.
         1. Create a build using `sg ci build main-dry-run` in order to get the CI to run the same exact checks it does on the `main` branch.
      1. No, but it seems to fail in step or code from another team.
         1. Reach out a member of the team responsible for that test.
         2. go for a. or b. from the previous points.
   1. No, and there is suspicion of a flake.
      - **Yes**: that's a flake. See the [Spotted a flake scenario](#spotted-a-flake)

### Builds are all failing on the `main` branch with the same error

- Severity: _major_
- Impact: no commits are being deployed on DogFood and `sourcegraph.com` until the problem is resolved. Cutting a release is impossible.
- Possible causes:
  - A previous Pull Request introduced a change that causes a test to fail.
  - A previous Pull Request introduced a change that modified state in an unexpected way and broke the CI.
  - An external dependency is not available anymore and is causing builds to fail.
  - Some rate limiting API is throttling us and causing builds to fail.

#### Actions

1. Identify the error in common with the recent builds on [Buildkite](https://buildkite.com/sourcegraph/sourcegraph/builds?branch=main).
1. Find the build where the problem appeared for the first time.
   - ðŸ’¡ Often it's the first build that became red, but check that the error is the same to be sure.
1. Is this an external failure or an internal one?
   - ðŸ’¡ External failures are about downloading a dependency like a package in a script or a in a Dockerfile. Often they'll manifest in the form of an HTTP error.
   - ðŸ’¡ If unsure, ask for help on #dev-chat.
   - **Yes**, it's an external failure:
     1. See the [SSH into an agent scenario](#ssh-into-an-agent)
     1. Try to reproduce the faulty HTTP request so you can observe what's the problem. Is it the same failure?
        - **Yes**: Do you know how to fix it? If **no** escalate by creating an incident (`/incident` on Slack).
        - **No**: escalate by creating an incident (`/incident` on Slack).
   - **No**, it's an internal failure:
     1. Is it involving a faulty build environment in the agents? (a given tool is not found where it should have been present, or have incorrect version)
        - See the [SSH into an agent scenario](#ssh-into-an-agent)
     2. Try to find an agent that recently successfully ran the faulty step (look for a green build on the `main` branch)
        1. Can you see a difference? If **yes** take note.
     3. Do you know how to fix it?
        - **Yes**: apply the fix.
        - **No**: escalate by creating an incident (`/incident` on Slack).

### Build are failing on the `main` branch with different errors

- Severity: _major_
- Impact: no commits are being deployed on DogFood and `sourcegraph.com` until the problem is resolved. Cutting a release is impossible.
- Possible causes:
  - A previous Pull Request introduced a change that causes a test to fail.
  - An external dependency is not available anymore and is causing builds to fail under certain conditions.
  - Some rate limiting API is throttling us and causing builds to fail.

#### Actions

1. Escalate by creating an incident (`/incident` on Slack).
1. Get some help by pinging `@dev-infra-support` on Slack in the #buildkite-main or #discuss-dev-infra channels.

### Builds are all failing in my branch, on Bazel jobs, with many timeouts or cache/disk related errors or container errors.

- Severity: _major_
- Impact: no commits are being deployed on DogFood and `sourcegraph.com` until the problem is resolved. Cutting a release is impossible.
- Possible causes:
  - A previous Pull Request introduced a change that causes a test to fail. If that's the case you should see the problem on the `main` build corresponding to the commit you branched out from.
  - A previous Pull Request introduced a change that modified state in an unexpected way and broke the CI. If that's the case you should see the problem on the `main` build corresponding to the commit you branched out from.
  - A previous build did not properly teardown containers used in e2e test suites.
  - Agents are in a corrupted state due to a previous build.
  - Agents ran out of disk space.

#### Actions

1. Escalate by creating an incident (`/incident` on Slack).
1. Get some help by pinging `@dev-infra-support` on Slack in the #buildkite-main or #discuss-dev-infra channels.
1. Request access to the DevX Day2Day entitle bundle by typing `/access_request` in Slack.
1. Restart the agents by scaling the corresponding deployment to 0 then to 2 again.
   - `kubectl scale --replicas=0 -n buildkite-bazel deployments/buildkite-agent-bazel`
   - Observe the pods count going down.
   - `kubectl scale --replicas=2 -n buildkite-bazel deployments/buildkite-agent-bazel`
   - The agent autoscaler will adjust the final replicas count on its own.
1. If you saw cache releated errors in the job logs, restart the remote-cache by scaling the corresponding deployment to 0 then to 1 again.
   - `kubectl scale --replicas=0 -n buildkite-bazel deployments/ci-bazel-remote-cache`
   - Observe the pods count going down.
   - `kubectl scale --replicas=1 -n buildkite-bazel deployments/ci-bazel-remote-cache`
   - Do not scale it above 1 instance, it uses a persistent disk that can only be accessed by a single instance.

### Spotted a flake

- Severity: _minor_
- Impact: Some builds will fail randomly, creating noise and slowing down the engineering team
- Possible causes:
  - Tests relying on timing.
  - Race conditions.
  - End to end tests are delicate by nature and can fail randomly due to the complexity of all involved components.

#### Actions

1. What kind of step is failing?

- Is this an End-to-end tests?
  - ðŸ’¡ E2E tests are fragile by nature, there is no way around it.
  - Take note.
- Is this a Docker image build step?
  - ðŸ’¡ This should really not be happening.
  - Is the error about the Docker daemon?
    - **Yes**, this is a CI infrastructure flake. Ping `@dev-infra-support` on Slack in the #buildkite-main or #discuss-dev-infra channels.
    - **No**: reach out to the team owning that Docker image _immediately_.
- Anything else
  - Take note of the failing step and go to next point.

1. Is that flake related to the CI infrastructure?

- The CI infrastructure often involves:
  - Docker daemon not being reachable.
  - Missing tools that we use to run the steps, such as `go`, `node`, `comby`, ...
  - Errors from `asdf`, which is used to manage the above tools.
- **Yes**: ping `@dev-infra-support` on Slack in the #buildkite-main or #discuss-dev-infra channels.
  - If nodoby is online to help:
    - Reach out for help in #dev-chat

1. Is that flake related to the code:

- See the process describe in the [flaky tests page](https://docs.sourcegraph.com/dev/background-information/testing_principles#flaky-tests)

### Is this a failure or a flake?

- Gravity: _minor_
- Impact: Some builds will fail randomly, creating noise and slowing down the engineering team
- Possible causes:
  - Tests relying on timing.
  - Race conditions.
  - End to end tests are delicate by nature and can fail randomly due to the complexity of all involved components.

#### Actions

1. Immediately restart the faulty step.
   - ðŸ’¡ It will save you time while you're looking at the logs.
   - Is the step passing now?
     - **Yes**: See [Spotted a flake scenario](#spotted-a-flake)
     - **No**: Give it another try, and see next point.
1. Check on [Grafana](https://sourcegraph.grafana.net/explore?orgId=1&left=%5B%22now-12h%22,%22now%22,%22grafanacloud-sourcegraph-logs%22,%7B%22refId%22:%22A%22,%22expr%22:%22%7Bapp%3D%5C%22buildkite%5C%22%7D%22%7D%5D) if there are any occurrences of the failures that were previously observed:
1. Go the the "Explore" section
1. Make sure to select `grafanacloud-sourcegraph-logs` in the dropdown at the top of page.
1. Scope the time window to `7 Days` to make sure to find previous occurrences if there are any
1. Enter a query such as `{app="buildkite"} |= "your error message"` where "your error message" is a string that identiy approximately the failure cause observed in the failing step.
1. Is there a build that failed exactly like this?
   - **Yes**:
     1. ðŸ’¡ Double check that you're looking at that the same step by inspecting the labels of message (click on the line to make them visible)
     1. **Yes**, that's a flake. See the [Spotted a flake scenario](#spotted-a-flake)
   - **No**: it's not a flake, reach out the team owning those tests.

You can also refer to the [Loom walkthrough "how to find out if a CI failure is a recurring flake"](https://www.loom.com/share/58cedf44d44c45a292f650ddd3547337).

### Builds are not being created on Buildkite

- Severity: _major_
- Impact: It's possible to merge a PR without going through CI. No builds are produced and it's impossible to deploy the new commits.
- Possible causes:
  - GitHub is experiencing some outage that is affecting webhooks.
  - Buildkite is experiencing some outage.
  - Webhooks that trigger the builds have been deleted.

#### Actions

1. Inspect [webhooks status](https://github.com/sourcegraph/sourcegraph/settings/hooks) on the `sourcegraph/sourcegraph` repository settings
1. If you're not authorized to see this page, ping `@dev-infra-support` or escalate to `@github-owners`.
1. Check the status of the webhook, if it's not green, something is wrong. However, if it is green it is no guarantee that the webhook is operating as usual! If GitHub Webhooks is experiencing degraded performance, it might not be emitting events to the endpoint at all any more, and the green status was the last submission before the outage started. See the next step to verify the status of Webhooks.
1. Check [GitHub Status](https://www.githubstatus.com/)
1. Check [Buildkite Status](https://www.buildkitestatus.com/)
1. A possible way to mitigate a GitHub outage is to recreate the webhook.
1. Delete the old buildkite webhook.
1. Create a new one by following these [instructions](https://buildkite.com/sourcegraph/sourcegraph/settings/setup/github).

### SSH into an agent

- Gravity: none
- Impact: none (unless a destructive action is performed)
- Possible cause:
  - Need to investigate a problem and suspect the agent is at fault

#### Actions

1. Identify if you want to look at a Bazel agent or a stateless one. Bazel agents are under the `buildkite-bazel` namespace, and stateless agents are under `buildkite` namespace.
1. Request access to the DevX Day2Day entitle bundle by typing `/access_request` in Slack.
1. Find the pod you want to SSH into with one of the following methods:
   1. Use `kubectl get pods -n $NAMESPACE -w` to observe the currently running agents and get the pod name (`k9s` works here too).
   2. From a Buildkite build page, click the "Timeline" tab of a job and see the entry for "Accepted Job". The "Host name" in the entry is also the name of the pod that the job was assigned to.
1. Use `kubectl exec -n $NAMESPACE -it buildkite-agent-xxxxxxxxxx-yyyyy -- bash` to open a shell on the Buildkite agent.

### Replacing Agents

- Gravity: _minor_
- Impact: May fail ongoing builds, but that's fine.
- Possible causes:
  - Newer version of the agents needs to be deployed.

#### Actions

1. Refer to the instructions [here](https://sourcegraph.com/github.com/sourcegraph/infrastructure/-/blob/buildkite/kubernetes/buildkite-agent-stateless/README.md#dispatched-agents) to remove currently deployed agents. The [buildkite-job-dispatcher](https://sourcegraph.com/github.com/sourcegraph/infrastructure/-/tree/docker-images/buildkite-job-dispatcher) will deploy jobs with any updated config.

### Agent availability issues

- Gravity: _major_
- Impact: Builds stuck in "waiting for agent"
- Possible cause:
  - Agent dispatch malfunction or GCP infrastructure outage

#### Actions

1. Check [dispatcher dashboard](https://console.cloud.google.com/monitoring/dashboards/builder/a87f3cbb-4d73-476d-8736-f3bc1ca9f234?folder=true&organizationId=true&project=sourcegraph-ci&dashboardBuilderState=%257B%2522editModeEnabled%2522:false%257D&timeDomain=1h) for health metrics
2. Check [dispatched agents](<https://console.cloud.google.com/kubernetes/workload/overview?folder=true&organizationId=true&project=sourcegraph-ci&pageState=(%22savedViews%22:(%22i%22:%22d63788ab9603422da3abba5f06030393%22,%22c%22:%5B%5D,%22n%22:%5B%22buildkite%22%5D),%22workload_list_table%22:(%22f%22:%22%255B%257B_22k_22_3A_22Is%2520system%2520object_22_2C_22t_22_3A11_2C_22v_22_3A_22_5C_22False_~*false_5C_22_22_2C_22i_22_3A_22is_system_22%257D_2C%257B_22k_22_3A_22_22_2C_22t_22_3A10_2C_22v_22_3A_22_5C_22buildkite-agent-stateless-_5C_22_22%257D%255D%22))>) for availability issues
3. Check [dispatcher logs](https://console.cloud.google.com/logs/query;lfeCustomFields=jsonPayload%252FexpectDispatch;query=resource.type%3D%22k8s_container%22%0Aresource.labels.project_id%3D%22sourcegraph-ci%22%0Aresource.labels.location%3D%22us-central1-c%22%0Aresource.labels.cluster_name%3D%22default-buildkite%22%0Aresource.labels.namespace_name%3D%22buildkite%22%0Aresource.labels.container_name%3D%22buildkite-job-dispatcher%22;summaryFields=jsonPayload%252FrunID,jsonPayload%252Fmsg,jsonPayload%252F%2522metric.agentsDispatched%2522,jsonPayload%252F%2522metric.freeAgents%2522,jsonPayload%252F%2522metric.scheduledJobs%2522,jsonPayload%252F%2522metric.totalAgents%2522:false:18:end:true;timeRange=PT30M?project=sourcegraph-ci) for details

For more details, see the source: [buildkite-job-dispatcher](https://github.com/sourcegraph/infrastructure/blob/main/buildkite/kubernetes/buildkite-job-dispatcher/README.md)
